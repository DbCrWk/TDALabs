{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do all of the imports and setup inline plotting\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from TDA import *\n",
    "import scipy.io.wavfile\n",
    "\n",
    "def getSlidingWindow(x, dim, Tau):\n",
    "    N = len(x)\n",
    "    NWindows = N-dim+1\n",
    "    X = np.zeros((NWindows, dim))\n",
    "    maxIndex = 0\n",
    "    for i in range(NWindows):\n",
    "        if len(x[i:i+Tau*dim:Tau]) < dim:\n",
    "            #If the window goes out of bounds, simply break\n",
    "            #and discard those windows\n",
    "            break\n",
    "        X[i, :] = x[i:i+Tau*dim:Tau]\n",
    "        maxIndex += 1\n",
    "    return X[0:maxIndex, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Biphonation Overview</h1>\n",
    "\n",
    "Biphonation refers to the presence of two or more simultaneous frequencies in a signal which are \"incommensurate\"; that is, their frequencies are linearly independent over the rational numbers.  In other words, the frequencies are \"inharmonic.\"  We saw a synthetic example in class 1 of cos(x) + cos(pi x).  Today, we will examine how this manifests itself in biology with horse whinnies that occur during states of high emotional valence.  During the steady state of a horse whinnie, biphonation is found\n",
    "\n",
    "<table>\n",
    "<tr><td>\n",
    "<img src = \"Whinnie.png\">\n",
    "</td></tr>\n",
    "<tr><td>\n",
    "<b>Figure 1</b>: Audio of a horse whinnie.  Courtesy of <a href = \"http://www.nature.com/articles/srep09989#s1\">http://www.nature.com/articles/srep09989#s1</a></td></tr>\n",
    "</table>\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/f8DdGpHkzu4\" frameborder=\"0\" allowfullscreen></iframe>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115921 / 115921 possible edges added (100%)\n"
     ]
    }
   ],
   "source": [
    "F0 = 493 #First fundamental frequency\n",
    "G0 = 1433 #Second fundamental frequency\n",
    "\n",
    "#Read in the audio file.  Fs is the sample rate, and\n",
    "#X is the audio signal\n",
    "Fs, X = scipy.io.wavfile.read(\"srep09989-s2.wav\")\n",
    "time = 0.585\n",
    "iStart = int(round(time*Fs))\n",
    "x = X[iStart:iStart+512]\n",
    "W = int(round(Fs/G0))\n",
    "\n",
    "Y = getSlidingWindow(x, W, 1)\n",
    "#Mean-center and normalize\n",
    "Y = Y - np.mean(Y, 0)[None, :]\n",
    "Y = Y/np.sqrt(np.sum(Y**2, 1))[:, None]\n",
    "\n",
    "PDs = doRipsFiltration(Y, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "lt.title(\"%g Seconds\"%time)\n",
    "plt.plot(x)\n",
    "plt.subplot(122)\n",
    "plotDGM(PDs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Music Analysis</h1>\n",
    "\n",
    "Music is full of repetition.  For instance, there is usually a hierarchy of rhythm which determines how the music \"pulses,\" or repeates itself in beat patterns.  Often, a dominant rhythm level is deemed the \"tempo\" of the music.  Typical tempos range from about 50 beats per minute to 200 beats per minute.  Let's take a moderate tempo level of 120 beats per minute, for instance, which occurs in the song \"Don't Stop Believin'\" by Journey.  This corresponds to a period of 0.5 seconds.  As we saw in the horse example, sound is sampled at 44100 samples per second.  This corresponds to an ideal sliding window interval length of 22050.  Let's try to compute the sliding window embedding of the raw audio to see if the tempo manifests itself with TDA.\n",
    "<BR><BR>\n",
    "<img src = \"journey.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Load Journey, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Audio Novelty Functions</h2>\n",
    "One way to deal with the fact that music is both messy and at a high sampling rate is to derive something called the \"audio novelty function,\" which is designed explicitly to pick up on rhythmic events.  \n",
    "\n",
    "[Show spectrogram with percussive broadband events]\n",
    "\n",
    "Let's now instead try our sliding window with the audio novelty function of the previous example instead of the raw audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use librosa to load in audio novelty functions, \n",
    "#play around with sliding window length again, see if matching tempo maximizes \n",
    "#the persistence\n",
    "#Also test speech data which is not periodic, and show that it's not possible under\n",
    "#similar variations in window lenght to recover any periodicity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
